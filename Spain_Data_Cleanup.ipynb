{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spain_Data_Cleanup.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMcChLwK0DVjgQ2ifGSfSEN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JMawyin/ClimateFamers/blob/main/Spain_Data_Cleanup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBASrTWXhzxZ"
      },
      "source": [
        "**Data Cleaning**\n",
        "\n",
        "We have a data set of different farm geographical boundaries in 5 different shapefiles. Also, we have a excel file with the dated farming practices in the same farms. The excel data was already cleaned, combined and exported into a csv file. It was easier to do this in excel than in Python.\n",
        "\n",
        "In this notebook the above data will be further processsed in a format more useful for analysis and machine learning:\n",
        "\n",
        "* Combined 5 shapefiles for 60+ farms into a single dataframe.\n",
        "* Add extra info in the shapefile dataframe such as farm size, country, state to aid downstream analysis.\n",
        "* Load farm practices csv into a dataframe and clean the data by:\n",
        " * Reduced the crop labels from crop + variant to only crop in English.\n",
        " * Simplified the farming practices.\n",
        "* Link the farm name in the shapefile/boundary dataframe to the farm practices dataframe by:\n",
        " * Change the case in the name string.\n",
        " * Filter out farm names not in both dataframes.\n",
        "* Save both dataframes into their own csv files. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS9nn1erlLvD"
      },
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcu47cssla5M"
      },
      "source": [
        "# **Loading Useful Libraries and Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YkR_PtN8T23",
        "outputId": "4cb1517b-8e93-4203-9817-4e5801a82f29"
      },
      "source": [
        "!pip install pyshp\n",
        "!pip install geopandas\n",
        "!pip install --upgrade reverse_geocoder"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyshp\n",
            "  Downloading pyshp-2.1.3.tar.gz (219 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 30 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 40 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 194 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 204 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 215 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 219 kB 5.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyshp\n",
            "  Building wheel for pyshp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyshp: filename=pyshp-2.1.3-py3-none-any.whl size=37325 sha256=b1e80e54b51f5a7bb1b4a37663b9f2c615f9a4669b5874354e0b33966d75347b\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/f8/87/53c8cd41545ba20e536ea29a8fcb5431b5f477ca50d5dffbbe\n",
            "Successfully built pyshp\n",
            "Installing collected packages: pyshp\n",
            "Successfully installed pyshp-2.1.3\n",
            "Collecting geopandas\n",
            "  Downloading geopandas-0.9.0-py2.py3-none-any.whl (994 kB)\n",
            "\u001b[K     |████████████████████████████████| 994 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting fiona>=1.8\n",
            "  Downloading Fiona-1.8.20-cp37-cp37m-manylinux1_x86_64.whl (15.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.4 MB 72 kB/s \n",
            "\u001b[?25hCollecting pyproj>=2.2.0\n",
            "  Downloading pyproj-3.1.0-cp37-cp37m-manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: shapely>=1.6 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.7.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.1.5)\n",
            "Collecting click-plugins>=1.0\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (7.1.2)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (21.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (2021.5.30)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (57.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->geopandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->geopandas) (1.19.5)\n",
            "Installing collected packages: munch, cligj, click-plugins, pyproj, fiona, geopandas\n",
            "Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.8.20 geopandas-0.9.0 munch-2.5.0 pyproj-3.1.0\n",
            "Collecting reverse_geocoder\n",
            "  Downloading reverse_geocoder-1.5.1.tar.gz (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from reverse_geocoder) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from reverse_geocoder) (1.4.1)\n",
            "Building wheels for collected packages: reverse-geocoder\n",
            "  Building wheel for reverse-geocoder (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for reverse-geocoder: filename=reverse_geocoder-1.5.1-py3-none-any.whl size=2268087 sha256=288d28b0f6d3c6d481c4a4d93e6e08f84d59e8b4b68ee7addce47b0a8f056d8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/6e/70/5423639428a2cac8ea7eb467214a4254b549b381f306a9c790\n",
            "Successfully built reverse-geocoder\n",
            "Installing collected packages: reverse-geocoder\n",
            "Successfully installed reverse-geocoder-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnO5t-9FQR_x"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import ee \n",
        "\n",
        "from glob import glob\n",
        "\n",
        "import shapefile #To read shapefiles into dataframe\n",
        "import geopandas as gpd\n",
        "import reverse_geocoder as rg\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "\n",
        "import altair as alt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoFJMO0UOfcu",
        "outputId": "51020f46-0b4d-4bd0-c309-504b8b73af77"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "Mounted at /content/drive\n",
            "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=gqKGbfniAXcn2JJTdhwYJK_oQuFQ8xB97D99B16Sb1k&code_challenge_method=S256\n",
            "\n",
            "The authorization workflow will generate a code, which you should paste in the box below. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "klgo6T8c65Ze"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYEh7sRZFGF7"
      },
      "source": [
        "#Functions\n",
        "\n",
        "#Find approximate country, state and city to centroid of farm\n",
        "def geo_loc(coord):\n",
        "  geo_loc = []\n",
        "  geo_loc = rg.search(coord)\n",
        "  geo_items = geo_loc[0].items()\n",
        "  geo_items = list(geo_items)\n",
        "  locations = []\n",
        "  locations = [a_tuple[1] for a_tuple in geo_items]\n",
        "  country = locations[5]\n",
        "  state = locations[3]\n",
        "  city = locations[2]\n",
        "  return country, state, city\n",
        "#Add country, state, city to columns in geodataframe\n",
        "def add_geo_loc(df):\n",
        "  df_area = df.copy()\n",
        "  df_area= df_area.to_crs({'init': 'epsg:3857'}) #change the projection to a Cartesian system (EPSG:3857, unit= m)\n",
        "  df_area[\"area_hectare\"] = df_area['geometry'].area/ 10000. #Adding area column in square km\n",
        "  df[\"Centroid_X_lon\"] = df[\"geometry\"].centroid.x\n",
        "  df[\"Centroid_Y_lat\"] = df[\"geometry\"].centroid.y\n",
        "  df[\"area_hectare\"] = df_area[\"area_hectare\"]\n",
        "  df[\"Country\"] = \"\"\n",
        "  df[\"State\"] = \"\"\n",
        "  df[\"City\"] = \"\"\n",
        "  for row in range(len(df)):\n",
        "    CC_loc = df.columns.get_loc(\"Country\")\n",
        "    Stt_loc = df.columns.get_loc(\"State\")\n",
        "    Cty_loc = df.columns.get_loc(\"City\")\n",
        "    coordinates = (df.iloc[row, 4],df.iloc[row, 3])\n",
        "    CC, State, City = geo_loc(coordinates)\n",
        "    df.iloc[row, CC_loc]= CC\n",
        "    df.iloc[row, Stt_loc]= State\n",
        "    df.iloc[row, Cty_loc]= City\n",
        "  return df\n",
        "\n",
        "#=========== Cloud Masking Functions\n",
        "\n",
        "def get_s2_sr_cld_col(aoi, start_date, end_date):\n",
        "    # Import and filter S2 SR.\n",
        "    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
        "        .filterBounds(aoi)\n",
        "        .filterDate(start_date, end_date)\n",
        "        .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER)))\n",
        "\n",
        "    # Import and filter s2cloudless.\n",
        "    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
        "        .filterBounds(aoi)\n",
        "        .filterDate(start_date, end_date))\n",
        "\n",
        "    # Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.\n",
        "    return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
        "        'primary': s2_sr_col,\n",
        "        'secondary': s2_cloudless_col,\n",
        "        'condition': ee.Filter.equals(**{\n",
        "            'leftField': 'system:index',\n",
        "            'rightField': 'system:index'\n",
        "        })\n",
        "    }))\n",
        "\n",
        "def add_cloud_bands(img):\n",
        "    # Get s2cloudless image, subset the probability band.\n",
        "    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
        "\n",
        "    # Condition s2cloudless by the probability threshold value.\n",
        "    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n",
        "\n",
        "    # Add the cloud probability layer and cloud mask as image bands.\n",
        "    return img.addBands(ee.Image([cld_prb, is_cloud]))\n",
        "\n",
        "def add_shadow_bands(img):\n",
        "    # Identify water pixels from the SCL band.\n",
        "    not_water = img.select('SCL').neq(6)\n",
        "\n",
        "    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
        "    SR_BAND_SCALE = 1e4\n",
        "    dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n",
        "\n",
        "    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
        "    shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n",
        "\n",
        "    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
        "    cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n",
        "        .reproject(**{'crs': img.select(0).projection(), 'scale': 100})\n",
        "        .select('distance')\n",
        "        .mask()\n",
        "        .rename('cloud_transform'))\n",
        "\n",
        "    # Identify the intersection of dark pixels with cloud shadow projection.\n",
        "    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n",
        "\n",
        "    # Add dark pixels, cloud projection, and identified shadows as image bands.\n",
        "    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))\n",
        "\n",
        "\n",
        "def add_cld_shdw_mask(img):\n",
        "    # Add cloud component bands.\n",
        "    img_cloud = add_cloud_bands(img)\n",
        "\n",
        "    # Add cloud shadow component bands.\n",
        "    img_cloud_shadow = add_shadow_bands(img_cloud)\n",
        "\n",
        "    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
        "    is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)\n",
        "\n",
        "    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
        "    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
        "    is_cld_shdw = (is_cld_shdw.focal_min(2).focal_max(BUFFER*2/20)\n",
        "        .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})\n",
        "        .rename('cloudmask'))\n",
        "\n",
        "    # Add the final cloud-shadow mask to the image.\n",
        "    return img_cloud_shadow.addBands(is_cld_shdw)\n",
        "\n",
        "\n",
        "def apply_cld_shdw_mask(img):\n",
        "    # Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n",
        "    not_cld_shdw = img.select('cloudmask').Not()\n",
        "\n",
        "    # Subset reflectance bands and update their masks, return the result.\n",
        "    return img.select('B.*').updateMask(not_cld_shdw)\n",
        "\n",
        "#Extract Google Earth Engine Polygon Geometry from geodataframe Polygon  \n",
        "def shp_ee_roi(poly_geom):\n",
        "  poly_list = list(poly_geom.exterior.coords)\n",
        "  x, y = zip(*poly_list)\n",
        "  XY_poly = list(zip(x, y))\n",
        "  ee_polygon = ee.Geometry.Polygon(XY_poly)\n",
        "  return ee_polygon\n",
        "\n",
        "#Image Collection Metadata to Dictionary\n",
        "def fc_to_dict(fc):\n",
        "  prop_names = fc.first().propertyNames()\n",
        "  prop_lists = fc.reduceColumns(\n",
        "      reducer=ee.Reducer.toList().repeat(prop_names.size()),\n",
        "      selectors=prop_names).get('list')\n",
        "  \n",
        "  return ee.Dictionary.fromLists(prop_names, prop_lists)\n",
        "\n",
        "#Image Collection Metadata to Dataframe\n",
        "def fc_to_df(fc):\n",
        "  prop_names = fc.first().propertyNames()\n",
        "  prop_lists = fc.reduceColumns(\n",
        "      reducer=ee.Reducer.toList().repeat(prop_names.size()),\n",
        "      selectors=prop_names).get('list')\n",
        "  Col_dict = ee.Dictionary.fromLists(prop_names, prop_lists)\n",
        "  df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in Col_dict.items() ]))\n",
        "  return df\n",
        "\n",
        "\n",
        "def getSentinelImages(roi: ee.geometry.Geometry, startDate: str, endDate: str, **kwargs) -> ee.ImageCollection:\n",
        "  \n",
        "  '''\n",
        "  startDate and endDate must be in the form \"YYYY-MM-DD\"\n",
        "\n",
        "  The current state of the function will only return images in which less than 20% of pixels\n",
        "  are labeled as cloudy pixels.\n",
        "  '''\n",
        "\n",
        "  return ee.ImageCollection('COPERNICUS/S2_SR').filterBounds(roi).filterDate(startDate, endDate)\\\n",
        ".filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', 10))\n",
        "\n",
        "#=========== Funtions for Indices Calculations\n",
        "def addNDVI(image: ee.image.Image) -> ee.image.Image:\n",
        "  ndvi = image.normalizedDifference(['B5', 'B4']).rename('NDVI');\n",
        "  return image.addBands(ndvi)\n",
        "\n",
        "#=====================================\n",
        "# Define a method for displaying Earth Engine image tiles to folium map.\n",
        "def add_ee_layer(self, ee_image_object, vis_params, name):\n",
        "  map_id_dict = ee.Image(ee_image_object).getMapId(vis_params)\n",
        "  folium.raster_layers.TileLayer(\n",
        "    tiles = map_id_dict['tile_fetcher'].url_format,\n",
        "    attr = \"Map Data © Google Earth Engine\",\n",
        "    name = name,\n",
        "    overlay = True,\n",
        "    control = True\n",
        "  ).add_to(self)\n",
        "\n",
        "\n",
        "def collectionMeans(image: ee.image.Image, index: str, geometry: ee.geometry.Geometry) -> ee.ImageCollection:\n",
        "\n",
        "  # Compute the mean of the passed index over the passed image\n",
        "  # the value is a dictionary, so get the index value from the dictionary\n",
        "  value = image.reduceRegion(**{\n",
        "    'geometry': geometry,\n",
        "    'reducer': ee.Reducer.mean(),\n",
        "  }).get(index)\n",
        "\n",
        "  # Adding computed index value\n",
        "  newFeature = ee.Feature(None, {\n",
        "      index : value\n",
        "  }).copyProperties(image, [\n",
        "      'system:time_start',\n",
        "      'SUN_ELEVATION'\n",
        "  ])\n",
        "\n",
        "  return newFeature\n",
        "\n",
        "# Function to add date variables to DataFrame.\n",
        "def add_date_info(df):\n",
        "  df['Timestamp'] = pd.to_datetime(df['system:time_start'], unit='ms')\n",
        "  df['Year'] = pd.DatetimeIndex(df['Timestamp']).year\n",
        "  df['Month'] = pd.DatetimeIndex(df['Timestamp']).month\n",
        "  df['Day'] = pd.DatetimeIndex(df['Timestamp']).day\n",
        "  df['DOY'] = pd.DatetimeIndex(df['Timestamp']).dayofyear\n",
        "  df['WOY'] = df['Timestamp'].dt.isocalendar().week#pd.Int64Index(idx.isocalendar().week)#pd.DatetimeIndex(df['Timestamp']).weekofyear\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvekhz8RlkEU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQv0DqPpllRc"
      },
      "source": [
        "# Loading csv data into dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nnjQ4R2mbSs"
      },
      "source": [
        "## Loading farm boundaries data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY6YMisaOS8t"
      },
      "source": [
        "We have 5 shapefiles containing boundary farm boundary information. To combine them all, first we load into a list all the shapefile filenames.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2apAIDd7FPx"
      },
      "source": [
        "shape_path = '/content/drive/MyDrive/Climate_Farmers/Datasets/Spain Elisabet/Shapes_Fincas/'\n",
        "shape_files = glob(shape_path + '*.shp')\n",
        "shape_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "byQyL1WR8hDR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na6ODG-3OhI8"
      },
      "source": [
        "Loading one of the shapefiles into a dataframe to see the structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vWGe-2xEQai"
      },
      "source": [
        "\n",
        "shp_df = gpd.read_file(shape_files[1])\n",
        "shp_df = add_geo_loc(shp_df)\n",
        "print(shp_df.shape)\n",
        "shp_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0HbBvZ0OpOj"
      },
      "source": [
        "Then we load all the shapefiles into a single dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDPRQwl2l9tA"
      },
      "source": [
        "file_num = len(shape_files)\n",
        "appended_farm_geo = []\n",
        "for i in range(file_num):\n",
        "    shp_df = gpd.read_file(shape_files[i])\n",
        "    shp_df = add_geo_loc(shp_df)\n",
        "    # store DataFrame in list\n",
        "    appended_farm_geo.append(shp_df)\n",
        "# see pd.concat documentation for more info\n",
        "appended_farm_geo = pd.concat(appended_farm_geo)\n",
        "#appended_farm_geo.to_csv (r'C:\\Users\\Ron\\Desktop\\export_dataframe.csv', index = False, header=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY0mCE6-q3FL"
      },
      "source": [
        "There are 73 different farm boundaries included in the original data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELgUS39Sq1jM"
      },
      "source": [
        "appended_farm_geo.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Umx4mbpmOEU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qH_GgKumMJo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeBGrYh9qGKh"
      },
      "source": [
        "# save_path = \"/content/drive/MyDrive/Climate_Farmers/Datasets/Spain Maria/\" + \"appended_farm_geo.csv\"\n",
        "# print(save_path)\n",
        "# appended_farm_geo.to_csv (save_path, index = False, header=True)\n",
        "appended_farm_geo.drop(['ID', 'id'], axis=1, inplace=True)\n",
        "appended_farm_geo.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yn2103IqyLA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwG4vSfImWk3"
      },
      "source": [
        "## Loading farm practices data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbjT5fsWPkNL"
      },
      "source": [
        "The farm practices info was initially given in a single excel file with multiple tabs. Some cleaning and combining was already done by hand in Excel before exporting the data as a csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZpCd6z4xZXO"
      },
      "source": [
        "practices_timeline_pth = \"/content/drive/MyDrive/Climate_Farmers/Datasets/Spain Elisabet/Datos_FincasV2.csv\"\n",
        "practices_timeline_df = pd.read_csv(practices_timeline_pth)\n",
        "practices_timeline_df.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VnjHJZumlQV"
      },
      "source": [
        "# Cleaning up farm practice data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULzlNELrmwIE"
      },
      "source": [
        "### Cleaning up Crop types.\n",
        "\n",
        "We see below that the CULTIVO (Crop) column is separates crops not only by type but also by variant. The absorption/radiative signal of a crop variant may not be different enough to increase the labelling complexity.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWGbiG3XOfmw"
      },
      "source": [
        "\n",
        "g = sns.catplot(y=\"CULTIVO\", data=practices_timeline_df, order = practices_timeline_df['CULTIVO'].value_counts().index, kind=\"count\", height=10, aspect=1.5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkfLrZhioC2X"
      },
      "source": [
        "The code below simply checks from a list of complete strings (crop names) in the column Cultivo. If found, a different string is added to a new column labeled Crop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4YjT2xWtggQ"
      },
      "source": [
        "pract_clean_timel_df = practices_timeline_df.copy()\n",
        "crop_lookup = {'TRIGO DURO AMILCAR': 'Trigo', 'TRITICALES BONDADOSO':'Tritical', 'CEBADA PLANET':'Cebada', 'GUISANTES':'Guisantes',\n",
        " 'GIRASOL':'Girasol', 'GARBANZOS':'Garbanzos', 'TRIGO ARTUR NICK': 'Trigo', 'TRIGO TOCAYO': 'Trigo', 'GIRASOL HH‐106':'Girasol',\n",
        " 'GUISANTES KAYANNE':'Guisantes', 'TRIGO KIKO NICK': 'Trigo', 'TRIGO MULHACEN': 'Trigo', 'TRIGO AVISPA': 'Trigo',\n",
        " 'GARBANZOS ITUCHI':'Garbanzos', 'TRIGO ARTURNICK': 'Trigo', 'TRIGO BLANDO ARTUR NICK': 'Trigo',\n",
        " 'TRIGO BLANDO ACORAZADO': 'Trigo', 'TRITICALE':'Tritical', 'ALTRAMUZ':'Altramuz', 'TRIGO BLANDO': 'Trigo',\n",
        " 'TRIGO DURO': 'Trigo', 'CEBADA':'Cebada', 'AVENA':'Avena'}\n",
        "pract_clean_timel_df['Crop'] = pract_clean_timel_df.CULTIVO.map(crop_lookup)\n",
        "pract_clean_timel_df.head(5)\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "g = sns.catplot(y=\"Crop\", data=pract_clean_timel_df, order = pract_clean_timel_df['Crop'].value_counts().index, kind=\"count\", height=7, aspect=1.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH7MjlbQo3c9"
      },
      "source": [
        "### Cleaning up Practices types\n",
        "\n",
        "The Labores (Farm Practices) column has many duplicates including many cases of incorrect spelling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QZHm_tPQklc"
      },
      "source": [
        "g = sns.catplot(y=\"LABORES\", data=practices_timeline_df, order = practices_timeline_df['LABORES'].value_counts().index, kind=\"count\", height=10, aspect=1.5)\n",
        "#g.set_xticklabels(rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhNXtC_Au6Ji"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMGEFwBeQi6R"
      },
      "source": [
        "The code below checks for a string fragment (APLICACION HERBICIDA would match ERB) and creates a new column with a new name. This sections has some guesswork as some terms for farming practices in Spanish are difficult to translate to English. For example:\n",
        "\n",
        "Pase de grada = ?\n",
        "\n",
        "Pase de rulo = ?\n",
        "\n",
        "Pase de regabina = ?\n",
        "\n",
        "Events such as application of Fertilizer occurred more than once. This is why the Fertilizer count is greater than 73 number of farm boundaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNdzdP9Q1pJm"
      },
      "source": [
        "\n",
        "pract_clean_timel_df['Practices'] = pract_clean_timel_df['LABORES']\n",
        "\n",
        "# map search string to update string\n",
        "labor_mapping = {'ERB': 'Herbicide', 'RADA Trek': 'Grade', 'OSECHA':'Harvest', \n",
        "                 'FUNGI':'Fungicide', 'ABON':'Fertilizer', 'SIEM':'Sowing',\n",
        "                 'GRADA':'Grada', 'RULO':'Rulo', 'CHISEL':'Chisel', 'INSEC':'Insecticide',\n",
        "                 'ESCA':'Scarifier', 'REGA':'Regabina','VIBRO':'Vibrocultivator'}\n",
        "\n",
        "# iterate mapping items\n",
        "for k, v in labor_mapping.items():\n",
        "    pract_clean_timel_df.loc[pract_clean_timel_df['LABORES'].str.contains(k), 'Practices'] = v\n",
        "\n",
        "pract_clean_timel_df.head(10)\n",
        "\n",
        "g = sns.catplot(y=\"Practices\", data=pract_clean_timel_df, order = pract_clean_timel_df['Practices'].value_counts().index, kind=\"count\", height=7, aspect=1.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL-idPtm6wrH"
      },
      "source": [
        "pract_clean_timel_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQq1W0Nvr3p9"
      },
      "source": [
        "### Filling empty fields in Farm Name and Crop Columns\n",
        "\n",
        "Only the first row per farm name and crop type is filled in the table. The function below takes that first string value and repeats it until it finds a new farm name and crop type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmmiEurd8lKD"
      },
      "source": [
        "##Repeats the last string for those column with rows containing empty strings.\n",
        "def Empty_FN_fixV2(df, column):\n",
        "  col_loc = df.columns.get_loc(column)\n",
        "  label_num = 1\n",
        "  for row in range(len(df)):\n",
        "    string = df.iloc[row, col_loc]\n",
        "    if string:\n",
        "      farm_name = string\n",
        "      continue\n",
        "    else:\n",
        "      df.iloc[row, col_loc] = farm_name\n",
        "      label_num += 1\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbyUQq838oXl"
      },
      "source": [
        "pract_clean_timel_df_V2 = pract_clean_timel_df.copy()\n",
        "pract_clean_timel_df_V2 = pract_clean_timel_df_V2.replace(np.nan, '', regex=True)\n",
        "pract_clean_timel_df_V2 = Empty_FN_fixV2(pract_clean_timel_df_V2, \"PARCELAS\")\n",
        "pract_clean_timel_df_V2 = Empty_FN_fixV2(pract_clean_timel_df_V2, \"Crop\")\n",
        "#pract_clean_timel_df_V2 = pract_clean_timel_df_V2[['FECHA','Grupo','PARCELAS','Crop','Practices','PRODUCTO']]\n",
        "pract_clean_timel_df_V2.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZjb8jWd0j0l"
      },
      "source": [
        "## Finding farms in both Boundary and Practices dataframe\n",
        "\n",
        "Both dataframes have a slightly different set of farm names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftV3Tcah_hGf"
      },
      "source": [
        "farm_pract_lst = pract_clean_timel_df_V2['PARCELAS'].unique()\n",
        "farm_pract_lst = [each_string.upper() for each_string in farm_pract_lst]\n",
        "print(\"The number of different farms in the Practices dataframe is: \",len(farm_pract_lst))\n",
        "#farm_pract_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xk4AKo6sweg"
      },
      "source": [
        "farm_bound_lst = appended_farm_geo['Name'].unique()\n",
        "farm_bound_lst = [each_string.upper() for each_string in farm_bound_lst]\n",
        "print(\"The number of different farms in the Boundaries dataframe is: \",len(farm_bound_lst))\n",
        "#farm_bound_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEzGmCUVzkXe"
      },
      "source": [
        "Here we find the farm names that are not in both lists. \n",
        "\n",
        "The syntax: \n",
        "\n",
        "numpy.setdiff1d(arr1, arr2)\n",
        "\n",
        "Finds the set difference of two arrays and return the unique values in arr1 that are not in arr2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSp7z0X5tVjb"
      },
      "source": [
        "farms_diff_1 = np.setdiff1d(farm_pract_lst,farm_bound_lst)\n",
        "print(\"Farms in Practices that are not in Boundaries:\", farms_diff_1)\n",
        "farms_diff_2 = np.setdiff1d(farm_bound_lst,farm_pract_lst)\n",
        "print(\"Farms in Boundaries that are not in Practices:\", farms_diff_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sv8DMGw1hAF"
      },
      "source": [
        "Here is the list of farm names that exists in both dataframes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da-gDj5WyAQ8"
      },
      "source": [
        "farm_pract_lst = set(farm_pract_lst)\n",
        "farm_intersection = farm_pract_lst.intersection(farm_bound_lst)\n",
        "#Find common elements of set and list\n",
        "farm_intersection = list(farm_intersection)\n",
        "print(\"There are\", len(farm_intersection),\"common farms in both datasets.\")\n",
        "farm_intersection.sort()\n",
        "farm_intersection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cNWCwnC14ab"
      },
      "source": [
        "## Filtering both dataframes to only include common farm names."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM0TOkcN2_sq"
      },
      "source": [
        "Here we change the case of the farm name in both Practices and Boundaries to upper case so that matches the list of common farms. Then we filter out rows that do not have have an entry in the list of common farms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLMxnUV53Fxa"
      },
      "source": [
        "Practices_filt = pract_clean_timel_df_V2.copy()\n",
        "Practices_filt['PARCELAS'] = [each_string.upper() for each_string in Practices_filt['PARCELAS']]\n",
        "print(\"Original number of rows in Practices:\",len(Practices_filt))\n",
        "Practices_filt = Practices_filt.loc[Practices_filt['PARCELAS'].isin(farm_intersection)]\n",
        "print(\"Filtered number of rows in Practices:\",len(Practices_filt))\n",
        "\n",
        "Boundaries_filt = appended_farm_geo.copy()\n",
        "Boundaries_filt['Name'] = [each_string.upper() for each_string in Boundaries_filt['Name']]\n",
        "print(\"Original number of rows in Boundaries:\",len(Boundaries_filt))\n",
        "Boundaries_filt = Boundaries_filt.loc[Boundaries_filt['Name'].isin(farm_intersection)]\n",
        "print(\"Filtered number of rows in Boundaries:\",len(Boundaries_filt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPJ9Cjdf5dkE"
      },
      "source": [
        "# Saving Data into csv files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFZ9ZkWU-qf8"
      },
      "source": [
        "Finally we save both dataframes into two different csv files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2JuQztK5kTB"
      },
      "source": [
        "folder_pth = \"/content/drive/MyDrive/Climate_Farmers/Datasets/Spain Elisabet/\"\n",
        "practice_pth = folder_pth + \"Practices_filt.csv\"\n",
        "print(practice_pth)\n",
        "Practices_filt.to_csv (practice_pth, index = False, header=True)\n",
        "\n",
        "boundary_pth = folder_pth + \"Boundaries_filt.csv\"\n",
        "print(boundary_pth)\n",
        "Boundaries_filt.to_csv (boundary_pth, index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymnpttYI_KIj"
      },
      "source": [
        "test_df = pd.read_csv(boundary_pth)\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}